

from flask import Flask, request, Response
from dotenv import load_dotenv
import os
import mysql.connector
import json
from datetime import datetime, timedelta
from decimal import Decimal
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
import keras
from tensorflow.keras.utils import get_custom_objects
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import (Conv1D, Bidirectional, LSTM, LayerNormalization, Dense, Concatenate)
from tensorflow.keras.optimizers import AdamW

## ë°ì´í„° ì˜ˆì¸¡ ì†Œìš”ì‹œê°„ ì¸¡ì •
import tracemalloc  # ì¶”ê°€

# â‘  BahdanauAttention ì •ì˜ (custom layer)
class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V  = tf.keras.layers.Dense(1)
    def call(self, values, query):
        hidden = tf.expand_dims(query, 1)
        score  = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden)))
        weights= tf.nn.softmax(score, axis=1)
        context= tf.reduce_sum(weights * values, axis=1)
        return context, weights

# â‘¡ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì½”ë“œ (í•™ìŠµ ë•Œ ì“°ì‹  ê²ƒê³¼ ì™„ì „íˆ ë™ì¼í•˜ê²Œ)
inp = Input(shape=(60,11), name="input")

x = Conv1D(128,3,activation='tanh',padding='same', name="conv1d_tanh")(inp)
x = Bidirectional(LSTM(64, return_sequences=True), name="bilstm_1")(x)
x = LayerNormalization(name="ln_1")(x)
x = Bidirectional(LSTM(32, return_sequences=True), name="bilstm_2")(x)
x = LayerNormalization(name="ln_2")(x)
x = Bidirectional(LSTM(16, return_sequences=True), name="bilstm_3")(x)

# ë§ˆì§€ë§‰ Bi-LSTM (return_state=True)
lstm_out, f_h, f_c, b_h, b_c = Bidirectional(
    LSTM(8, return_sequences=True, return_state=True),
    name="bilstm_4"
)(x)
state_h = Concatenate()([f_h, b_h])

# Bahdanau ì–´í…ì…˜ ì ìš©
context, attn_w = BahdanauAttention(16)(lstm_out, state_h)

# ì¶œë ¥
out = Dense(3, activation='linear', name="output")(context)
model = Model(inp, out, name="conv_lstm_rnn_attention")

# â‘¢ ì»´íŒŒì¼ (í•™ìŠµ ë•Œ ì“°ì‹  ì„¤ì •ê³¼ ë™ì¼í•˜ê²Œ)
model.compile(
    loss='mse',
    optimizer=AdamW(learning_rate=0.00025, weight_decay=0.004, beta_1=0.95),
    metrics=[tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.RootMeanSquaredError()]
)

# ì´ íŒŒì¼(app.py)ì´ ìœ„ì¹˜í•œ ë””ë ‰í„°ë¦¬ êµ¬í•˜ê¸°
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

#  ë°ì´í„° í´ë”ê¹Œì§€ì˜ ìƒëŒ€ ê²½ë¡œ ì—°ê²°
PIPELINE_PATH = os.path.join(BASE_DIR, 'data', 'feature_pipelines_v2.pkl')
WEIGHTS_PATH = os.path.join(BASE_DIR, 'data', 'my_lstm_model_30s_v5.weights.h5')
# ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ
model.load_weights(WEIGHTS_PATH)

load_dotenv()
app = Flask(__name__)

# ë©”ëª¨ë¦¬, ì‘ë‹µì‹œê°„ ì¸¡ì • ì‹œì‘
tracemalloc.start() 



# âœ… DB ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
db_config = {
    'host': os.getenv('MYSQL_HOST'),
    'port': int(os.getenv('MYSQL_PORT')),
    'user': os.getenv('MYSQL_USER'),
    'password': os.getenv('MYSQL_PASSWORD'),
    'database': os.getenv('MYSQL_DATABASE')
}
# ì €ì¥ëœ íŒŒì´í”„ë¼ì¸ ë¶ˆëŸ¬ì˜¤ê¸°
feature_pipelines = joblib.load(PIPELINE_PATH)


# âœ… JSON ì§ë ¬í™” ì§€ì› í•¨ìˆ˜
def dustPred(sensor_results):
    def convert(obj):
        if isinstance(obj, (datetime, timedelta)):
            return str(obj)
        if isinstance(obj, Decimal):
            return float(obj)
        return obj

    # âœ… ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
    cleaned_results = []
    for row in sensor_results:
        cleaned_row = {k: convert(v) for k, v in row.items()}
        cleaned_results.append(cleaned_row)
    
    df_results = pd.DataFrame.from_dict(cleaned_results)
    df_results['dtime'] = pd.to_datetime(df_results['dtime'])
    df_results = df_results.set_index('dtime').sort_index()

    # â€” (B) ì¸ë±ìŠ¤(Timestamp)ë¡œë¶€í„° ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± â€”â€”â€”
    doy = df_results.index.dayofyear  # 1~365 (ìœ¤ë…„ì´ë©´ 366)

    #  365ì¼ ì£¼ê¸°ë¡œ ì‚¬ì¸/ì½”ì‚¬ì¸ ê³„ì‚°
    df_results['doy_sin'] = np.sin(2 * np.pi * doy / 365)
    df_results['doy_cos'] = np.cos(2 * np.pi * doy / 365)

    #  í•˜ë£¨ ì¤‘ ê²½ê³¼ëœ ì´ˆ(sec_of_day) ê³„ì‚° â†’ ì‚¬ì¸/ì½”ì‚¬ì¸
    sec_of_day = (
        df_results.index.hour * 3600
        + df_results.index.minute * 60
        + df_results.index.second
    )
    df_results['time_sin'] = np.sin(2 * np.pi * sec_of_day / 86400)
    df_results['time_cos'] = np.cos(2 * np.pi * sec_of_day / 86400)

    df = df_results[['pm10', 'pm25', 'pm1', 'temp', 'humidity', 'co2den', 'atmospheric_press', 'doy_sin','doy_cos','time_sin','time_cos']].copy()
    df.columns = ['PM10','PM2_5','PM1','Temp','Humidity','CO2Den', 'AtmosphericPress', 'doy_sin','doy_cos','time_sin','time_cos']

    # ë¯¸ì„¸ë¨¼ì§€, ëŒ€ê¸°ì•• ë¡œê·¸ ë³€í™˜
    col = ['PM10', 'PM2_5', 'PM1', 'CO2Den']
    for j in range(len(col)):
        df[col[j]+'_log'] = np.log1p(df[col[j]])

    # ì‚¬ìš©í•  ì›”(month) ì§€ì •
    month = 12
    pipeline = feature_pipelines[month]

    thresholds = pipeline['thresholds']
    input_scaler = pipeline['scaler']
    y_scaler = pipeline['y_scaler']

    # ì´ìƒì¹˜ ì²˜ë¦¬
    for feat, (low, high) in thresholds.items():
        df[feat] = df[feat].clip(lower=low, upper=high)
    features = ['PM10_log', 'PM2_5_log', 'PM1_log', 'Temp', 'Humidity', 'AtmosphericPress', 'CO2Den_log']
    # ì…ë ¥ ìŠ¤ì¼€ì¼ë§
    df[features] = input_scaler.transform(df[features])
    
    X = df[['PM10_log','PM2_5_log','PM1_log','Temp','Humidity','CO2Den_log', 'AtmosphericPress', 'doy_sin','doy_cos','time_sin','time_cos']].to_numpy()


    # ì˜ˆì¸¡ (ìŠ¤ì¼€ì¼ëœ y)
    y_pred_scaled = model.predict(X.reshape(1, *X.shape))

    # ì˜ˆì¸¡ ê²°ê³¼ ì›ë³¸ ë‹¨ìœ„ë¡œ ë³µì›
    y_pred = y_scaler.inverse_transform(y_pred_scaled).tolist()

    return Response(
        json.dumps(y_pred, ensure_ascii=False),
        status=200,
        mimetype='application/json'
    )

# âœ… ì„¼ì„œ ë°ì´í„° API
@app.route('/flask-station-db-test', methods=['GET'])
def db_test():
    st_id = request.args.get('st_id')
    # http://127.0.0.1:5000/flask-station-db-test?st_id=1
    try:
        st_id = int(st_id)      #st_id = 1 ë¡œ ê³ ì •ì •
    except Exception as e:
        return Response(
            json.dumps({'error': 'Invalid st_id'}, ensure_ascii=False),
            status=400,
            mimetype='application/json'
        )

    try:
    # âœ… ìš”ì¼ í™•ì¸
        weekday_eng = datetime.today().strftime('%A').lower()
        print("[ìš”ì¼ í™•ì¸]:", weekday_eng)

        # âœ… í˜„ì¬ ì‹œê° êµ¬í•˜ê¸° (hh:mm:ss í˜•ì‹)
        current_time_str = datetime.now().strftime('%H:%M:%S')
        print("[í˜„ì¬ ì‹œê° ê¸°ì¤€]:", current_time_str)
        print("\n=== ğŸ“Œ [API í˜¸ì¶œ ì‹œê°] ===", datetime.now())
        t_start = datetime.now()

        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor(dictionary=True, buffered=True)

        # âœ… í˜„ì¬ ì‹œê°„ë³´ë‹¤ ì´ì „ì˜ 600ê°œ ë°ì´í„° ì¡°íšŒ
        cursor.execute("""
            SELECT * FROM sensor
            WHERE st_id = %s AND weekday = %s AND time_hms <= %s
            ORDER BY time_hms DESC
            LIMIT 60
        """, (st_id, weekday_eng, current_time_str))

        results = cursor.fetchall()
        print("[ì¿¼ë¦¬ ê²°ê³¼ ê°œìˆ˜]:", len(results))

        cursor.close()
        conn.close()

        if results:
            return dustPred(results)
        else:
            return Response(
                json.dumps({'error': 'í•´ë‹¹ ì¡°ê±´ì˜ sensor ë°ì´í„° ì—†ìŒ'}, ensure_ascii=False),
                status=404,
                mimetype='application/json'
            )

    except Exception as e:
        print("[DB ë˜ëŠ” ì¿¼ë¦¬ ì˜¤ë¥˜ ë°œìƒ]:", str(e))
        return Response(
            json.dumps({'error': str(e)}, ensure_ascii=False),
            status=500,
            mimetype='application/json'
        )

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)